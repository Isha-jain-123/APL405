# -*- coding: utf-8 -*-
"""testing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h6YoISzOnwmRFmfHng4-_QYH2HP6usHD
"""

import numpy as np
from scipy.optimize import minimize
import pandas as pd
from matplotlib import pyplot as plt

url_train = 'https://raw.githubusercontent.com/APL405/APL405/main/Week_04_Multiclass%20Classification/fashion-mnist_train.csv'
url_test = 'https://raw.githubusercontent.com/APL405/APL405/main/Week_04_Multiclass%20Classification/fashion-mnist_test.csv'
train = pd.read_csv(url_train)
test = pd.read_csv(url_test)

class costing:
    J_hist=[]
    def data_clean(self,data): # 'data' is a pandas dataframe
      X = data.values[:,1:] ; y = data.values[:,0]
      X = (X - np.min(X)) / (np.max(X) - np.min(X))
      return X, y

    def sigmoid(self,z):
      g = 1 / (1 + np.exp(-z))  
      return g  

    def costFunctionReg(self,w,X,y,lambda_):
      if y.dtype==bool:
        y=y.astype(int)
      m = y.shape[0]
      h = self.sigmoid(np.dot(X,w.T))
      J = (-np.dot(y,np.log(h)) - np.dot((1-y),np.log((1-h))))/m + (lambda_/(2*m)) * np.dot(w,w.T) - (lambda_/(2*m))*np.dot(w[0],w[0].T)
      grad = np.zeros(w.shape[0])
      grad[0] = (np.dot((h-y).T,X[:,0]))/m
      grad[1:] = (np.dot((h-y).T,X[:,1:]))/m + (lambda_/m)*w[1:]
      
      return J, grad


    def predictOneVsAll(self,all_w,X,num_labels):
      m,n = X.shape;
      X = np.concatenate([np.ones((m, 1)), X], axis=1)

      p = self.sigmoid(np.dot(X, all_w.T))
      pred = np.argmax(p, axis=1)
      return pred


    def minCostFun(self, train_data): #'train_data' is a pandas dataframe
      X, y = self.data_clean(train_data)
      m,n = X.shape
      num_labels = 10
      lambda_ = 0.1         # Regularization parameter
      iters = 4000
      
      X_ = np.concatenate([np.ones((m, 1)), X], axis=1)
      all_w =  np.zeros((num_labels, n + 1))             # Intialize 'w' for all classes as zero


      for i in range(num_labels):
        w_ini = np.zeros((n+1,1))
        y_i = np.array((y==i))
        res = minimize(self.costFunctionReg,w_ini,args=(X_, y_i, lambda_), method='CG',jac=True ,options={"maxiter" : iters})
        all_w[i, :] = res.x

      # J,grad = self.costFunctionReg(all_w, X_,y,lambda_)
      # self.J_hist.append(J)

      p = self.predictOneVsAll(all_w,X, 10)
      acrcy =  np.mean(y==p)*100 
      
      return all_w,acrcy

    def TestingAccu(self, test_data,all_w): #'test_data' is a pandas dataframe
      X_test,y_test=self.data_clean(test_data)
      acrcy_test = np.mean(self.predictOneVsAll(all_w,X_test,10)==y_test)*100   # Training set accuracy (in %) rounded off to 3 decimal places (Ans ~ 86.667)
      acrcy_test = round(acrcy_test,3)
      return acrcy_test

model = costing()
all_w,acrcy = model.minCostFun(train)
acrcy_test = model.TestingAccu(test,all_w)
print(acrcy)
print(acrcy_test)

def displayData(X, example_width=None, figsize=(10, 10)):
    """
    Displays 2D data stored in X in a nice grid.
    """
    # Compute rows, cols
    if X.ndim == 2:
        m, n = X.shape
    elif X.ndim == 1:
        n = X.size
        m = 1
        X = X[None]  # Promote to a 2 dimensional array
    else:
        raise IndexError('Input X should be 1 or 2 dimensional.')

    example_width = example_width or int(np.round(np.sqrt(n)))
    example_height = n / example_width

    # Compute number of items to display
    display_rows = int(np.floor(np.sqrt(m)))
    display_cols = int(np.ceil(m / display_rows))

    fig, ax_array = plt.subplots(display_rows, display_cols, figsize=figsize)
    fig.subplots_adjust(wspace=0.025, hspace=0.025)

    ax_array = [ax_array] if m == 1 else ax_array.ravel()

    for i, ax in enumerate(ax_array):
        ax.imshow(X[i].reshape(example_width, example_width, order='F'),
                  cmap='Greys', extent=[0, 1, 0, 1])
        ax.axis('off')


X, y = model.data_clean(train)
rand_ind = np.random.choice(y.shape[0],6, replace=False) # Randomly select 6 data points to display
a = X[rand_ind, :]
print(y[rand_ind])

displayData(a)

X_test,y_test=costing().data_clean(test)
rand_ind2 = np.random.choice(y_test.shape[0],6, replace=False)
b=X_test[rand_ind2, :]
p=model.predictOneVsAll(all_w,X,10)
print(y_test[rand_ind2])
displayData(b)

cost_history = model.J_hist
print(cost_history)
plt.xlabel('Number of iterations')
plt.ylabel('Cost History')
plt.title('Plot of Cost History vs Number of iterations')
a = cost_history
b = np.array([i for i in range(len(cost_history))])
plt.plot(b,a)